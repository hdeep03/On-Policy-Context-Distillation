<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		/* background-color: #f0eee6; */
		background-color: #faf9f5;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 45%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color: #faf9f5;
		/* border-left: 1px solid #DDD; */
		/* border-right: 1px solid #DDD; */
		padding: 8px 30px;
		/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
		font-family: "Iowan Old Style", serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			font-family: "Iowan Old Style", serif;
			padding: 5px;
	}
	.margin-right-block {
			/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir"; */
			font-family: "Iowan Old Style", serif;
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 24px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#faf9f5;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>On-Policy Context Distillation</title>
      <meta property="og:title" content="On-Policy Context Distillation" />
			<meta charset="UTF-8">
  </head>

  <body>
		<div class="content-margin-container" style="padding-top: 30px;">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block" style="display: flex; flex-direction: column; align-items: center;">
						<table class="header" align="center" style="margin-left:auto; margin-right:auto;">
								<tr>
									<td colspan=4 align="center">
										<span style="font-size: 32px; font-family: 'Iowan Old Style', serif;">On-Policy Context Distillation</span>
									</td>
								</tr>
								<tr>
										<td align="right">
												<span style="font-size:17px"><a href="https://hdeep.dev/">Harsh Deep</a></span>
										</td>
										<td align="center"></td> <!-- empty cell to center the names -->
										<td align="left">
												<span style="font-size:17px"><a href="https://www.linkedin.com/in/mar-pham/">Maria Pham</a></span>
										</td>
								</tr>
								<tr>
									<td colspan=4 align="center"><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#prior_work">Prior work</a><br><br>
              <a href="#experiments_and_methodology">Experiments and Methodology</a><br><br>
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Introduction</h1>

				Large language models (LLMs) exhibit strong <em>in-context learning</em> abilities: when provided with demonstrations, tools, or long instructional scaffolds in the prompt, they can rapidly adapt to new tasks without any change to their parameters. While powerful, this reliance on long contexts is expensive, brittle, and often impractical in deployment. Every additional example or instruction increases inference cost and latency, and maintaining carefully engineered prompts can be cumbersome.

				<br><br>

				<em>Context distillation</em> addresses this problem by compressing the information contained in long prompts into a model’s weights. In context distillation, a <em>teacher</em> model is given a rich context (e.g., many-shot demonstrations), while a <em>student</em> model is trained to reproduce the teacher’s behavior using little or no context at all. The goal is to retain the benefits of in-context learning while eliminating the need to supply long prompts at inference time.

				<br><br>
			</div>
			<div class="margin-right-block">
			</div>
	</div>

		<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<img src="./images/sft_context_distillation.svg" width=512px/>
		</div>
		<div class="margin-right-block">
			<strong>Figure 1: Supervised Fine-Tuning Context Distillation.</strong> Teacher forcing is used such that the student receives the teacher's output sequence as input at each step rather than using the student's own generated sequence. 
		</div>
	</div>

	<div class="content-margin-container" id="prior_work">
				<div class="margin-left-block">
				</div>
			<div class="main-content-block">
					<h1>Prior work</h1>
				Most existing work on context distillation relies on <em>supervised fine-tuning</em> (SFT). In this off-policy setup, trajectories are first sampled from a teacher model conditioned on a long context, and the student is then trained to imitate these fixed teacher-generated samples. Because the student never generates its own trajectories during training, learning is restricted to the states visited by the teacher. While effective, this off-policy formulation can be brittle: small deviations by the student at inference time may lead it into parts of the sequence space that were never observed during training, causing errors to compound over long generations.

				<br><br>

				In parallel, recent work in post-training has explored <em>on-policy distillation</em> as a more robust alternative to both supervised fine-tuning and reinforcement learning. In on-policy distillation, the student samples trajectories from its own policy and receives dense, per-token supervision from a stronger teacher. This approach combines the relevance of on-policy learning, which trains the model on the states it actually visits, with the sample efficiency of distillation, avoiding the sparse and unstable rewards common in reinforcement learning.

				<br><br>

				Despite these advances, on-policy distillation has so far been studied primarily in standard post-training settings, such as instruction following or reasoning, where the teacher and student receive identical inputs. To our knowledge, it has not been applied to <em>context distillation</em>, where the core challenge is to transfer behaviors that only emerge when a long context is present.

				<br><br>

				In this work, we investigate on-policy distillation as a training paradigm for context distillation. We ask whether sampling trajectories from the student—while using a teacher equipped with a long context to provide supervision—can more faithfully and efficiently transfer context-dependent behaviors than traditional off-policy approaches. We also explore how to isolate and distill the <em>incremental effect</em> of the long context, rather than the teacher’s entire policy, and show that on-policy methods provide a natural and effective framework for doing so.
				</div>
		    <div class="margin-right-block">
						<!-- Margin note that clarifies some detail #main-content-block for intro section. -->
		    </div>
		</div>
		<div class="content-margin-container" id="experiments_and_methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Experiments and Methodology</h1>
					<h3>On-Policy Context Distillation</h3>
					We propose on-policy context distillation as a more stable and effective alternative to supervised fine tuning for context distillation. Rather than training the student exclusively on teacher generated data, the student actively samples trajectories from its own policy, ensuring that learning is grounded in the distribution of states the model will actually encounter during inference. For every trajectory the student produces, a stronger teacher model provides dense, per token supervision that guides the student toward higher quality reasoning and response generation.
					This framework retains the relevance of on-policy learning, since the student directly improves on the behaviors it naturally exhibits, while also preserving the efficiency and stability of distillation. By relying on rich, token level feedback rather than the sparse and unstable rewards common in reinforcement learning, on-policy context distillation creates a more reliable and efficient training signal. The result is a training approach that brings together the strengths of on-policy learning and distillation to achieve more robust context dependent behavior without the instability often associated with reinforcement learning methods.					<br><br>
			</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<img src="./images/on_policy_context_distillation.svg" width=512px/>
		</div>
		<div class="margin-right-block">
			<strong>Figure 2: On-policy Context Distillation.</strong> A teacher model generates token-level outputs for questions with detailed context, the student model samples its own trajectories, a reward function compares the two, and importance sampling updates the student's weights based on on-policy supervision.
		</div>
	</div>

		<div class="content-margin-container" id="experiments_and_methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h3>Context-Gain Reward Function</h3>
					We propose a context-gain-based reward function that is more sensitive to the incremental effect of the long context. Specifically, we define the per-token reward as:
					<br><br>
					<div style="text-align: center;">
						<math xmlns="http://www.w3.org/1998/Math/MathML">
						  <mrow>
							<mi>R</mi>
							<mo stretchy="false">(</mo>
							<mi>x</mi>
							<mo stretchy="false">)</mo>
							<mo>=</mo>
					  
							<mo>-</mo>
							<mo stretchy="false">[</mo>
					  
							<mi>log</mi>
							<mo>&#x2061;</mo>
							<mi>&#x3C0;</mi>
							<msub>
							  <mi></mi>
							  <mi>&#x03B8;</mi>
							</msub>
							<mo stretchy="false">(</mo>
							<mi>x</mi>
							<mo stretchy="false">)</mo>
					  
							<mo>-</mo>
					  
							<mo stretchy="false">(</mo>
					  
							<mi>log</mi>
							<mo>&#x2061;</mo>
							<mi>&#x3C0;</mi>
							<msub>
							  <mi></mi>
							  <mrow>
								<mi>student</mi>
								<mo>,</mo>
								<mi>base</mi>
							  </mrow>
							</msub>
							<mo stretchy="false">(</mo>
							<mi>x</mi>
							<mo stretchy="false">)</mo>
					  
							<mo>+</mo>
					  
							<mi>log</mi>
							<mo>&#x2061;</mo>
							<mi>&#x3C0;</mi>
							<msub>
							  <mi></mi>
							  <mrow>
								<mi>teacher</mi>
								<mo>,</mo>
								<mi>long</mi>
							  </mrow>
							</msub>
							<mo stretchy="false">(</mo>
							<mi>x</mi>
							<mo stretchy="false">)</mo>
					  
							<mo>-</mo>
					  
							<mi>log</mi>
							<mo>&#x2061;</mo>
							<mi>&#x3C0;</mi>
							<msub>
							  <mi></mi>
							  <mrow>
								<mi>teacher</mi>
								<mo>,</mo>
								<mi>short</mi>
							  </mrow>
							</msub>
							<mo stretchy="false">(</mo>
							<mi>x</mi>
							<mo stretchy="false">)</mo>
					  
							<mo stretchy="false">)</mo>
					  
							<mo stretchy="false">]</mo>
						  </mrow>
						</math>
					  </div>
					  <br>
					  This reward encourages the student to preserve its base behavior while incorporating only the incremental effect induced by the teacher’s long context, measured relative to a no-context teacher baseline. When the teacher and student are different models, this formulation helps isolate the contribution of the context itself, rather than transferring the teacher’s stylistic preferences or other model-specific artifacts that are unrelated to the contextual information.
					  <br><br>
					  We hypothesize that this reward will outperform the baseline most clearly in settings where the teacher and student differ in architecture or scale, and where the student's learning capacity is constrained, such as when using low-rank LoRA adapters.
					  <br><br>
			</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
